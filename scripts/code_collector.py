#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æ™ºèƒ½ä»£ç æ”¶é›†å·¥å…·
æä¾›ä¸€é”®æ‰¹é‡å¯¼å…¥ã€æ™ºèƒ½ç‰‡æ®µæå–ã€è‡ªåŠ¨ç»“æ„å›¾ç”Ÿæˆç­‰åŠŸèƒ½
"""

import os
import json
import sys
import argparse
from pathlib import Path
from typing import List, Dict, Tuple, Optional, Set
from datetime import datetime
import re

# å°è¯•å¯¼å…¥ code_cleaner æ¨¡å—
try:
    from code_cleaner import clean_content_deeply, remove_comments, extract_code_skeleton, is_junk_filename
    HAS_CODE_CLEANER = True
except ImportError:
    HAS_CODE_CLEANER = False

# Windows ç¼–ç ä¿®å¤
if sys.platform == 'win32':
    if sys.stdout.encoding != 'utf-8':
        import codecs
        sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
        sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')


class CodeCollector:
    """æ™ºèƒ½ä»£ç æ”¶é›†å™¨"""

    def __init__(self, project_path: str, config: Dict = None):
        """åˆå§‹åŒ–æ”¶é›†å™¨"""
        self.project_path = Path(project_path).resolve()
        self.config = config or {}
        self.collected_files = []
        self.structure_tree = {}
        self.existing_md_data = None  # ç”¨äºå­˜å‚¨å·²æœ‰çš„ Markdown è§£æç»“æœ
        # ä»£ç æ¸…æ´—é€‰é¡¹
        self.clean_mode = self.config.get('clean_mode', 'none')  # none, comments, skeleton
        self.remove_junk = self.config.get('remove_junk', True)  # æ˜¯å¦ç§»é™¤åƒåœ¾æ–‡ä»¶

    def batch_import(self, file_paths: List[str]) -> Dict:
        """
        ä¸€é”®æ‰¹é‡å¯¼å…¥å¤šä¸ªæ–‡ä»¶

        å‚æ•°:
            file_paths: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆç›¸å¯¹æˆ–ç»å¯¹è·¯å¾„ï¼‰

        è¿”å›:
            {
                "files": [{"path": "...", "content": "...", "language": "...", "lines": 100}, ...],
                "structure": "æ ‘å½¢ç»“æ„å­—ç¬¦ä¸²",
                "stats": {"total_files": 10, "total_lines": 1000, "languages": {...}},
                "skipped_files": [{"path": "...", "reason": "..."}, ...]
            }
        """
        result = {
            "files": [],
            "structure": "",
            "stats": {"total_files": 0, "total_lines": 0, "languages": {}},
            "skipped_files": []
        }

        collected_paths = []
        max_size_kb = self.config.get('max_file_size_kb', 500)

        for file_path in file_paths:
            abs_path = self._resolve_path(file_path)
            if not abs_path.exists() or not abs_path.is_file():
                continue

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size_kb = abs_path.stat().st_size / 1024
            if file_size_kb > max_size_kb:
                result["skipped_files"].append({
                    "path": str(file_path),
                    "reason": f"æ–‡ä»¶è¿‡å¤§ ({file_size_kb:.1f} KB > {max_size_kb} KB)",
                    "size_kb": file_size_kb,
                    "lines": self._count_lines(abs_path)
                })
                continue

            # è¯»å–æ–‡ä»¶å†…å®¹
            content, encoding = self._read_file_safely(abs_path)
            if content is None:
                result["skipped_files"].append({
                    "path": str(file_path),
                    "reason": "ç¼–ç é”™è¯¯ï¼Œæ— æ³•è¯»å–"
                })
                continue

            # è·å–ç›¸å¯¹è·¯å¾„
            try:
                rel_path = abs_path.relative_to(self.project_path)
            except ValueError:
                rel_path = abs_path

            # åƒåœ¾æ–‡ä»¶è¿‡æ»¤
            if self.remove_junk and HAS_CODE_CLEANER:
                if is_junk_filename(str(rel_path)):
                    result["skipped_files"].append({
                        "path": str(file_path),
                        "reason": "åƒåœ¾æ–‡ä»¶ï¼ˆè‡ªåŠ¨è¿‡æ»¤ï¼‰"
                    })
                    continue

            # ä»£ç æ¸…æ´—
            if HAS_CODE_CLEANER and self.clean_mode != 'none':
                ext = abs_path.suffix.lower()
                if self.clean_mode == 'comments':
                    content = remove_comments(content, ext)
                elif self.clean_mode == 'skeleton':
                    content = extract_code_skeleton(content, ext)

            # è¯­è¨€æ£€æµ‹
            language = self._detect_language(abs_path)
            lines = len(content.splitlines())

            result["files"].append({
                "path": str(rel_path),
                "content": content,
                "language": language,
                "lines": lines,
                "size_kb": abs_path.stat().st_size / 1024
            })

            collected_paths.append(rel_path)

            # æ›´æ–°ç»Ÿè®¡
            result["stats"]["total_lines"] += lines
            result["stats"]["languages"][language] = result["stats"]["languages"].get(language, 0) + 1

        result["stats"]["total_files"] = len(result["files"])

        # ç”Ÿæˆç›®å½•ç»“æ„
        result["structure"] = self._generate_tree_structure(collected_paths)

        return result

    def extract_snippets(self, file_path: str, ranges: List[Dict]) -> Dict:
        """
        æ™ºèƒ½ç‰‡æ®µæå–ï¼ˆæ”¯æŒè¡Œå·èŒƒå›´å’Œå‡½æ•°åï¼‰

        å‚æ•°:
            file_path: æ–‡ä»¶è·¯å¾„
            ranges: æå–èŒƒå›´åˆ—è¡¨
                [
                    {"type": "lines", "start": 10, "end": 50},
                    {"type": "function", "name": "calculate_total"},
                    {"type": "class", "name": "UserModel"}
                ]

        è¿”å›:
            {
                "file_path": "...",
                "snippets": [
                    {"type": "lines", "range": "10-50", "content": "...", "lines": 41},
                    {"type": "function", "name": "calculate_total", "content": "...", "lines": 15}
                ],
                "total_lines": 56
            }
        """
        abs_path = self._resolve_path(file_path)
        if not abs_path.exists():
            return {"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}

        content, _ = self._read_file_safely(abs_path)
        if content is None:
            return {"error": f"æ— æ³•è¯»å–æ–‡ä»¶: {file_path}"}

        lines = content.splitlines()
        result = {
            "file_path": str(file_path),
            "snippets": [],
            "total_lines": 0
        }

        for range_spec in ranges:
            snippet = None

            if range_spec["type"] == "lines":
                # æŒ‰è¡Œå·æå–
                start = range_spec.get("start", 1) - 1  # è½¬æ¢ä¸º 0-based
                end = range_spec.get("end", len(lines))
                snippet_lines = lines[start:end]

                snippet = {
                    "type": "lines",
                    "range": f"{start + 1}-{end}",
                    "content": "\n".join(snippet_lines),
                    "lines": len(snippet_lines)
                }

            elif range_spec["type"] in ["function", "class", "method"]:
                # æŒ‰å‡½æ•°/ç±»åæå–
                name = range_spec.get("name")
                snippet_content, snippet_lines = self._extract_by_name(
                    content,
                    name,
                    range_spec["type"],
                    abs_path.suffix,
                    skeleton_mode=(self.clean_mode == 'skeleton')
                )

                if snippet_content:
                    snippet = {
                        "type": range_spec["type"],
                        "name": name,
                        "content": snippet_content,
                        "lines": snippet_lines
                    }

            if snippet:
                result["snippets"].append(snippet)
                result["total_lines"] += snippet["lines"]

        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯ï¼ˆç”¨äºåˆå¹¶æ¨¡å¼å…¼å®¹æ€§ï¼‰
        language = self._detect_language(abs_path)
        result["stats"] = {
            "total_files": 1,
            "total_lines": result["total_lines"],
            "languages": {language: 1}
        }

        # å°† snippets è½¬æ¢ä¸ºåˆå¹¶æ¨¡å¼å…¼å®¹çš„æ ¼å¼
        result["snippets"] = [{
            "file_path": result["file_path"],
            "snippets": result["snippets"]
        }]

        return result

    def parse_existing_markdown(self, md_path: str) -> Dict:
        """
        è§£æå·²æœ‰çš„ Markdown æ–‡ä»¶ï¼Œæå–ç»“æ„åŒ–æ•°æ®

        è¿”å›:
            {
                "header": "æ–‡ä»¶å¤´éƒ¨å†…å®¹",
                "files": {
                    "core": [{"path": "...", "content": "...", "language": "..."}],
                    "other": [{"path": "...", "content": "...", "language": "..."}]
                },
                "snippets": [{"file_path": "...", "snippets": [...]}],
                "structure": "ç›®å½•ç»“æ„å­—ç¬¦ä¸²",
                "stats": {...},
                "skipped_files": [...]
            }
        """
        if not os.path.exists(md_path):
            return None

        with open(md_path, 'r', encoding='utf-8') as f:
            content = f.read()

        result = {
            "header": "",
            "files": {"core": [], "other": []},
            "snippets": [],
            "structure": "",
            "stats": {},
            "skipped_files": []
        }

        # æå–æ–‡ä»¶å¤´éƒ¨ï¼ˆä»å¼€å§‹åˆ°ç¬¬ä¸€ä¸ª ## æ ‡é¢˜ï¼‰
        header_match = re.search(r'^(.*?)(?=^## )', content, re.MULTILINE | re.DOTALL)
        if header_match:
            result["header"] = header_match.group(1)

        # æå–ç›®å½•ç»“æ„
        structure_match = re.search(r'## ğŸ“ ç›®å½•ç»“æ„\s*\n\s*```(?:text)?\n(.*?)\n```', content, re.DOTALL)
        if structure_match:
            result["structure"] = structure_match.group(1)

        # æå–æ ¸å¿ƒæ–‡ä»¶
        core_section = re.search(r'## ğŸ¯ æ ¸å¿ƒæ–‡ä»¶\s*\n(.*?)(?=^## |\Z)', content, re.MULTILINE | re.DOTALL)
        if core_section:
            result["files"]["core"] = self._parse_file_sections(core_section.group(1))

        # æå–æ™®é€šæ–‡ä»¶
        other_section = re.search(r'## ğŸ“„ ä»£ç æ–‡ä»¶\s*\n(.*?)(?=^## |\Z)', content, re.MULTILINE | re.DOTALL)
        if other_section:
            result["files"]["other"] = self._parse_file_sections(other_section.group(1))

        # æå–ä»£ç ç‰‡æ®µ
        snippet_sections = re.finditer(r'## ğŸ“„ ä»£ç ç‰‡æ®µ: (.+?)\n(.*?)(?=^## |\Z)', content, re.MULTILINE | re.DOTALL)
        for match in snippet_sections:
            file_path = match.group(1)
            snippet_content = match.group(2)
            snippets = self._parse_snippet_sections(snippet_content)
            result["snippets"].append({"file_path": file_path, "snippets": snippets})

        # æå–è·³è¿‡çš„æ–‡ä»¶
        skipped_section = re.search(r'## âš ï¸ è·³è¿‡çš„æ–‡ä»¶\s*\n(.*?)(?=^## |\Z)', content, re.MULTILINE | re.DOTALL)
        if skipped_section:
            result["skipped_files"] = self._parse_skipped_files(skipped_section.group(1))

        # ç»Ÿè®¡ä¿¡æ¯ï¼šä»å·²è§£æå†…å®¹å›æ¨ï¼Œä¿è¯ append æ¨¡å¼ä¸‹å¯æ­£ç¡®ç´¯è®¡
        all_files = result["files"]["core"] + result["files"]["other"]
        snippet_groups = result["snippets"]

        total_lines = 0
        languages = {}

        for file_info in all_files:
            total_lines += len(file_info.get("content", "").splitlines())
            lang = file_info.get("language") or "text"
            languages[lang] = languages.get(lang, 0) + 1

        for snippet_group in snippet_groups:
            file_lang = self._detect_language(Path(snippet_group.get("file_path", "unknown")))
            languages[file_lang] = languages.get(file_lang, 0) + 1
            for snippet in snippet_group.get("snippets", []):
                total_lines += len(snippet.get("content", "").splitlines())

        result["stats"] = {
            "total_files": len(all_files) + len(snippet_groups),
            "total_lines": total_lines,
            "languages": languages
        }

        return result

    def _parse_file_sections(self, section_content: str) -> List[Dict]:
        """è§£ææ–‡ä»¶æ®µè½"""
        files = []
        file_matches = re.finditer(r'### File: (.+?)\n\s*```(\w+)?\n(.*?)\n```', section_content, re.DOTALL)
        for match in file_matches:
            files.append({
                "path": match.group(1),
                "language": match.group(2) or "text",
                "content": match.group(3)
            })
        return files

    def _parse_snippet_sections(self, section_content: str) -> List[Dict]:
        """è§£æä»£ç ç‰‡æ®µæ®µè½"""
        snippets = []

        # åŒ¹é…å‡½æ•°/ç±»ç‰‡æ®µ
        func_matches = re.finditer(r'### (Function|Class|Method): (.+?)\n\s*```.*?\n(.*?)\n```', section_content, re.DOTALL)
        for match in func_matches:
            snippets.append({
                "type": match.group(1).lower(),
                "name": match.group(2),
                "content": match.group(3)
            })

        # åŒ¹é…è¡ŒèŒƒå›´ç‰‡æ®µ
        line_matches = re.finditer(r'### è¡Œ (\d+-\d+)\n\s*```.*?\n(.*?)\n```', section_content, re.DOTALL)
        for match in line_matches:
            snippets.append({
                "type": "lines",
                "range": match.group(1),
                "content": match.group(2)
            })

        return snippets

    def _parse_skipped_files(self, section_content: str) -> List[Dict]:
        """è§£æè·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨"""
        skipped = []
        file_matches = re.finditer(r'### (.+?)\n(.*?)(?=### |$)', section_content, re.DOTALL)
        for match in file_matches:
            file_path = match.group(1)
            details = match.group(2)

            reason_match = re.search(r'\*\*åŸå› \*\*ï¼š(.+)', details)
            size_match = re.search(r'\*\*æ–‡ä»¶å¤§å°\*\*ï¼š(.+)', details)

            skipped.append({
                "path": file_path,
                "reason": reason_match.group(1) if reason_match else "æœªçŸ¥åŸå› ",
                "size_kb": float(size_match.group(1).split()[0]) if size_match else None
            })

        return skipped

    def merge_markdown_data(self, existing: Dict, new_data: Dict) -> Dict:
        """
        åˆå¹¶æ–°æ—§æ•°æ®

        å‚æ•°:
            existing: å·²æœ‰çš„è§£ææ•°æ®
            new_data: æ–°çš„æ•°æ®ï¼ˆæ¥è‡ª batch_import æˆ– extract_snippetsï¼‰

        è¿”å›:
            åˆå¹¶åçš„æ•°æ®ç»“æ„
        """
        if not existing:
            return new_data

        merged = {
            "header": existing["header"],  # ä¿ç•™åŸæœ‰å¤´éƒ¨
            "files": existing["files"].copy() if "files" in existing else {"core": [], "other": []},
            "snippets": existing["snippets"].copy() if "snippets" in existing else [],
            "structure": existing["structure"],
            "stats": existing["stats"].copy() if "stats" in existing else {},
            "skipped_files": existing["skipped_files"].copy() if "skipped_files" in existing else []
        }

        # åˆå¹¶æ–‡ä»¶åˆ—è¡¨ï¼ˆå»é‡ï¼‰
        if "files" in new_data:
            existing_paths = {f["path"] for f in merged["files"]["core"] + merged["files"]["other"]}

            for file_info in new_data["files"]:
                if file_info["path"] not in existing_paths:
                    if self._is_core_file(file_info["path"]):
                        merged["files"]["core"].append(file_info)
                    else:
                        merged["files"]["other"].append(file_info)

        # åˆå¹¶ä»£ç ç‰‡æ®µ
        if "snippets" in new_data:
            # æŸ¥æ‰¾æ˜¯å¦å·²æœ‰è¯¥æ–‡ä»¶çš„ç‰‡æ®µ
            existing_snippet_files = {s["file_path"]: i for i, s in enumerate(merged["snippets"])}

            for snippet_data in new_data["snippets"]:
                file_path = snippet_data["file_path"]
                if file_path in existing_snippet_files:
                    # åˆå¹¶åˆ°å·²æœ‰æ–‡ä»¶çš„ç‰‡æ®µåˆ—è¡¨ï¼ˆå»é‡ï¼‰
                    idx = existing_snippet_files[file_path]
                    existing_snippet_names = {
                        s.get("name") or s.get("range")
                        for s in merged["snippets"][idx]["snippets"]
                    }

                    for snippet in snippet_data["snippets"]:
                        snippet_id = snippet.get("name") or snippet.get("range")
                        if snippet_id not in existing_snippet_names:
                            merged["snippets"][idx]["snippets"].append(snippet)
                else:
                    # æ·»åŠ æ–°æ–‡ä»¶çš„ç‰‡æ®µ
                    merged["snippets"].append(snippet_data)

        # åˆå¹¶ç›®å½•ç»“æ„
        if "structure" in new_data and new_data["structure"]:
            merged["structure"] = self._merge_tree_structures(
                existing["structure"],
                new_data["structure"]
            )

        # æ›´æ–°è·³è¿‡çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆç§»é™¤å·²æˆåŠŸæå–çš„ï¼‰
        if "snippets" in new_data:
            extracted_files = {s["file_path"] for s in new_data.get("snippets", [])}
            merged["skipped_files"] = [
                s for s in merged["skipped_files"]
                if s["path"] not in extracted_files
            ]

        # æ·»åŠ æ–°è·³è¿‡çš„æ–‡ä»¶ï¼ˆå»é‡ï¼‰
        if "skipped_files" in new_data:
            existing_skipped_paths = {s["path"] for s in merged["skipped_files"]}
            for skipped in new_data["skipped_files"]:
                if skipped["path"] not in existing_skipped_paths:
                    merged["skipped_files"].append(skipped)

        # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        if "stats" in new_data:
            merged["stats"]["total_files"] = (
                len(merged["files"]["core"]) +
                len(merged["files"]["other"]) +
                len(merged["snippets"])
            )
            merged["stats"]["total_lines"] = existing["stats"].get("total_lines", 0) + new_data["stats"].get("total_lines", 0)

            # åˆå¹¶è¯­è¨€ç»Ÿè®¡
            merged["stats"]["languages"] = existing["stats"].get("languages", {}).copy()
            for lang, count in new_data["stats"].get("languages", {}).items():
                merged["stats"]["languages"][lang] = merged["stats"]["languages"].get(lang, 0) + count

        return merged

    def _merge_tree_structures(self, existing: str, new: str) -> str:
        """åˆå¹¶ä¸¤ä¸ªæ ‘å½¢ç›®å½•ç»“æ„"""
        if not existing:
            return new
        if not new:
            return existing

        # ç®€åŒ–å¤„ç†ï¼šå°†ä¸¤ä¸ªæ ‘åˆå¹¶ï¼ˆå®é™…åœºæ™¯ä¸­å¯ä»¥æ›´æ™ºèƒ½åœ°åˆå¹¶ï¼‰
        # æå–æ‰€æœ‰æ–‡ä»¶è·¯å¾„ï¼Œé‡æ–°ç”Ÿæˆæ ‘
        def extract_paths(tree_str: str) -> Set[str]:
            paths = set()
            for line in tree_str.split('\n'):
                # ç§»é™¤æ ‘å½¢å­—ç¬¦ï¼Œæå–è·¯å¾„
                clean_line = re.sub(r'[â”œâ””â”‚â”€\s]+', '', line).strip('/')
                if clean_line:
                    paths.add(clean_line)
            return paths

        existing_paths = extract_paths(existing)
        new_paths = extract_paths(new)
        all_paths = existing_paths | new_paths

        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œè¿”å›åŸæœ‰ç»“æ„ï¼ˆå®é™…å¯ä»¥é‡æ–°ç”Ÿæˆå®Œæ•´æ ‘ï¼‰
        return existing

    def generate_markdown(self, data: Dict, user_intent: str = "", append_mode: bool = False, existing_md_path: str = None) -> str:
        """
        ç”Ÿæˆ Markdown æ–‡æ¡£

        å‚æ•°:
            data: batch_import æˆ– extract_snippets è¿”å›çš„æ•°æ®
            user_intent: ç”¨æˆ·æ„å›¾æè¿°
            append_mode: æ˜¯å¦ä¸ºè¿½åŠ æ¨¡å¼ï¼ˆæ™ºèƒ½åˆå¹¶åˆ°å¯¹åº”åŒºåŸŸï¼‰
            existing_md_path: å·²æœ‰ Markdown æ–‡ä»¶è·¯å¾„ï¼ˆè¿½åŠ æ¨¡å¼éœ€è¦ï¼‰

        è¿”å›:
            å®Œæ•´çš„ Markdown å­—ç¬¦ä¸²
        """
        # è¿½åŠ æ¨¡å¼ï¼šè§£æå·²æœ‰æ–‡ä»¶å¹¶åˆå¹¶æ•°æ®
        if append_mode and existing_md_path:
            existing_data = self.parse_existing_markdown(existing_md_path)
            if existing_data:
                # åˆå¹¶æ•°æ®
                data = self.merge_markdown_data(existing_data, data)

        md_parts = []

        # æ ‡é¢˜å’Œå…ƒä¿¡æ¯
        project_name = self.project_path.name
        md_parts.append(f"# Project: {project_name}\n")

        # å…ƒä¿¡æ¯
        update_time = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        if append_mode and existing_md_path:
            md_parts.append(f"**æœ€åæ›´æ–°**: {update_time}\n")
        else:
            md_parts.append(f"**ç”Ÿæˆæ—¶é—´**: {update_time}\n")

        if user_intent:
            md_parts.append(f"**æ”¶é›†ç›®çš„**: {user_intent}\n")

        # é¡¹ç›®ç±»å‹ï¼ˆå¦‚æœæœ‰ï¼‰
        if "detected_project_type" in self.config:
            md_parts.append(f"**é¡¹ç›®ç±»å‹**: {self.config.get('project_type_name', 'æœªçŸ¥')}\n")

        md_parts.append("\n---\n\n")

        # ç›®å½•ç»“æ„
        if "structure" in data and data["structure"]:
            md_parts.append("## ğŸ“ ç›®å½•ç»“æ„\n\n")
            md_parts.append("```text\n")
            md_parts.append(data["structure"])
            md_parts.append("\n```\n\n---\n\n")

        # æ–‡ä»¶å†…å®¹ï¼ˆæ‰¹é‡å¯¼å…¥æ¨¡å¼ - æ”¯æŒåˆå¹¶åçš„æ•°æ®ï¼‰
        if "files" in data and isinstance(data["files"], dict):
            # å¤„ç†åˆå¹¶åçš„æ•°æ®ç»“æ„
            core_files = data["files"].get("core", [])
            other_files = data["files"].get("other", [])

            if core_files:
                md_parts.append("## ğŸ¯ æ ¸å¿ƒæ–‡ä»¶\n\n")
                for file_info in core_files:
                    md_parts.append(self._format_file_section(file_info))

            if other_files:
                md_parts.append("## ğŸ“„ ä»£ç æ–‡ä»¶\n\n")
                for file_info in other_files:
                    md_parts.append(self._format_file_section(file_info))
        elif "files" in data and isinstance(data["files"], list):
            # å¤„ç†åŸå§‹æ•°æ®ç»“æ„ï¼ˆé¦–æ¬¡ç”Ÿæˆï¼‰
            core_files = []
            other_files = []

            for file_info in data["files"]:
                if self._is_core_file(file_info["path"]):
                    core_files.append(file_info)
                else:
                    other_files.append(file_info)

            if core_files:
                md_parts.append("## ğŸ¯ æ ¸å¿ƒæ–‡ä»¶\n\n")
                for file_info in core_files:
                    md_parts.append(self._format_file_section(file_info))

            if other_files:
                md_parts.append("## ğŸ“„ ä»£ç æ–‡ä»¶\n\n")
                for file_info in other_files:
                    md_parts.append(self._format_file_section(file_info))

        # ä»£ç ç‰‡æ®µï¼ˆç‰‡æ®µæå–æ¨¡å¼ - æ”¯æŒåˆå¹¶åçš„æ•°æ®ï¼‰
        if "snippets" in data:
            if isinstance(data["snippets"], list) and len(data["snippets"]) > 0:
                # åˆå¹¶åçš„æ•°æ®ç»“æ„
                if isinstance(data["snippets"][0], dict) and "file_path" in data["snippets"][0]:
                    for snippet_group in data["snippets"]:
                        md_parts.append(f"## ğŸ“„ ä»£ç ç‰‡æ®µ: {snippet_group['file_path']}\n\n")
                        for snippet in snippet_group["snippets"]:
                            md_parts.append(self._format_snippet_section(snippet))
                # åŸå§‹æ•°æ®ç»“æ„ï¼ˆé¦–æ¬¡ç”Ÿæˆï¼‰
                else:
                    md_parts.append(f"## ğŸ“„ ä»£ç ç‰‡æ®µ: {data.get('file_path', 'æœªçŸ¥')}\n\n")
                    for snippet in data["snippets"]:
                        md_parts.append(self._format_snippet_section(snippet))

        # ç»Ÿè®¡ä¿¡æ¯
        if "stats" in data:
            md_parts.append("## ğŸ“Š ç»Ÿè®¡ä¿¡æ¯\n\n")
            stats = data["stats"]
            md_parts.append(f"- æ€»æ–‡ä»¶æ•°ï¼š{stats['total_files']}\n")
            md_parts.append(f"- æ€»ä»£ç è¡Œæ•°ï¼š{stats['total_lines']}\n")

            if stats.get("languages"):
                lang_stats = []
                total = sum(stats["languages"].values())
                for lang, count in sorted(stats["languages"].items(), key=lambda x: x[1], reverse=True):
                    pct = (count / total) * 100
                    lang_stats.append(f"{lang} ({pct:.1f}%)")
                md_parts.append(f"- ä¸»è¦è¯­è¨€ï¼š{', '.join(lang_stats)}\n")

        # è·³è¿‡çš„æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
        if "skipped_files" in data and data["skipped_files"]:
            md_parts.append("\n---\n\n")
            md_parts.append("## âš ï¸ è·³è¿‡çš„æ–‡ä»¶\n\n")
            md_parts.append("ä»¥ä¸‹æ–‡ä»¶å› ä½“ç§¯è¿‡å¤§æˆ–ç¼–ç é—®é¢˜æœªèƒ½è‡ªåŠ¨æ”¶é›†ï¼Œ**éœ€è¦ Agent æ‰‹åŠ¨å¤„ç†**ï¼š\n\n")
            for skipped in data["skipped_files"]:
                md_parts.append(f"### {skipped['path']}\n\n")
                md_parts.append(f"- **åŸå› **ï¼š{skipped['reason']}\n")
                if 'size_kb' in skipped:
                    md_parts.append(f"- **æ–‡ä»¶å¤§å°**ï¼š{skipped['size_kb']:.1f} KB\n")
                if 'lines' in skipped and skipped['lines']:
                    md_parts.append(f"- **é¢„ä¼°è¡Œæ•°**ï¼šçº¦ {skipped['lines']} è¡Œ\n")
                md_parts.append(f"- **å»ºè®®**ï¼šä½¿ç”¨ç‰‡æ®µæå–æ¨¡å¼ (--mode snippets) æŒ‡å®šå‡½æ•°/ç±»åæˆ–è¡Œå·èŒƒå›´\n\n")

        # ç”¨æˆ·æ„å›¾æ€»ç»“ï¼ˆæ–‡æœ«ï¼‰
        if user_intent:
            md_parts.append("\n---\n\n")
            md_parts.append("## ğŸ¯ æ”¶é›†ç›®çš„æ€»ç»“\n\n")
            md_parts.append(f"{user_intent}\n\n")
            md_parts.append("**æç¤º**ï¼šä»¥ä¸Šä»£ç å·²æ ¹æ®æ­¤ç›®çš„æ”¶é›†æ•´ç†ï¼Œå¯ç›´æ¥ç”¨äºç›¸å…³åˆ†ææˆ–å¼€å‘ä»»åŠ¡ã€‚\n")

        return "".join(md_parts)

    def _resolve_path(self, path: str) -> Path:
        """è§£æè·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹å’Œç»å¯¹è·¯å¾„ï¼‰"""
        p = Path(path)
        if p.is_absolute():
            return p
        return self.project_path / p

    def _read_file_safely(self, path: Path) -> Tuple[Optional[str], Optional[str]]:
        """å®‰å…¨è¯»å–æ–‡ä»¶ï¼ˆå°è¯•å¤šç§ç¼–ç ï¼‰"""
        encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']

        for encoding in encodings:
            try:
                with open(path, 'r', encoding=encoding) as f:
                    return f.read(), encoding
            except (UnicodeDecodeError, FileNotFoundError):
                continue

        return None, None

    def _count_lines(self, path: Path) -> Optional[int]:
        """å¿«é€Ÿç»Ÿè®¡æ–‡ä»¶è¡Œæ•°ï¼ˆä¸å®Œæ•´è¯»å–ï¼‰"""
        try:
            with open(path, 'rb') as f:
                return sum(1 for _ in f)
        except:
            return None

    def _detect_language(self, path: Path) -> str:
        """æ£€æµ‹ç¼–ç¨‹è¯­è¨€"""
        ext = path.suffix.lower()
        lang_map = {
            '.py': 'python', '.js': 'javascript', '.ts': 'typescript',
            '.jsx': 'jsx', '.tsx': 'tsx', '.java': 'java',
            '.cpp': 'cpp', '.c': 'c', '.h': 'c', '.hpp': 'cpp',
            '.cs': 'csharp', '.go': 'go', '.rs': 'rust',
            '.rb': 'ruby', '.php': 'php', '.swift': 'swift',
            '.kt': 'kotlin', '.scala': 'scala', '.r': 'r',
            '.m': 'objective-c', '.sql': 'sql', '.sh': 'bash',
            '.yaml': 'yaml', '.yml': 'yaml', '.json': 'json',
            '.xml': 'xml', '.html': 'html', '.css': 'css',
            '.scss': 'scss', '.sass': 'sass', '.md': 'markdown',
            '.vue': 'vue', '.svelte': 'svelte'
        }
        return lang_map.get(ext, 'text')

    def _generate_tree_structure(self, file_paths: List[Path]) -> str:
        """ç”Ÿæˆæ ‘å½¢ç›®å½•ç»“æ„"""
        if not file_paths:
            return ""

        # æ„å»ºæ ‘å½¢ç»“æ„
        tree = {}
        for path in file_paths:
            parts = path.parts
            current = tree
            for part in parts:
                if part not in current:
                    current[part] = {}
                current = current[part]

        # æ¸²æŸ“æ ‘å½¢ç»“æ„
        def render_tree(node: Dict, prefix: str = "", is_last: bool = True) -> List[str]:
            lines = []
            items = sorted(node.items())

            for i, (name, children) in enumerate(items):
                is_last_item = (i == len(items) - 1)

                if prefix == "":
                    lines.append(f"{name}/")
                    lines.extend(render_tree(children, "    ", is_last_item))
                else:
                    connector = "â””â”€â”€ " if is_last_item else "â”œâ”€â”€ "
                    is_file = len(children) == 0
                    display_name = name if is_file else f"{name}/"
                    lines.append(f"{prefix}{connector}{display_name}")

                    if children:
                        extension = "    " if is_last_item else "â”‚   "
                        lines.extend(render_tree(children, prefix + extension, is_last_item))

            return lines

        tree_lines = render_tree(tree)
        return "\n".join(tree_lines)

    def _extract_by_name(self, content: str, name: str, element_type: str, file_ext: str, skeleton_mode: bool = False) -> Tuple[Optional[str], int]:
        """æ ¹æ®å‡½æ•°/ç±»åæå–ä»£ç 

        å‚æ•°:
            content: æ–‡ä»¶å†…å®¹
            name: å‡½æ•°/ç±»å
            element_type: ç±»å‹ (function, class, method)
            file_ext: æ–‡ä»¶æ‰©å±•å
            skeleton_mode: æ˜¯å¦ä½¿ç”¨éª¨æ¶æ¨¡å¼ï¼ˆä»…æå–å£°æ˜ï¼Œå»é™¤å®ç°ï¼‰
        """
        lines = content.splitlines()

        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©æ­£åˆ™æ¨¡å¼
        if file_ext in ['.py']:
            if element_type == 'function':
                pattern = rf'^\s*def\s+{re.escape(name)}\s*\('
            elif element_type == 'class':
                pattern = rf'^\s*class\s+{re.escape(name)}\s*[\(:]'
            else:
                return None, 0
        elif file_ext in ['.js', '.ts', '.jsx', '.tsx', '.html', '.htm', '.vue']:
            if element_type == 'function':
                # æ›´ç²¾ç¡®çš„å‡½æ•°å®šä¹‰æ¨¡å¼ï¼Œæ’é™¤å‡½æ•°è°ƒç”¨
                pattern = rf'(^\s*function\s+{re.escape(name)}\s*\(|^\s*async\s+function\s+{re.escape(name)}\s*\(|^\s*const\s+{re.escape(name)}\s*=|^\s*let\s+{re.escape(name)}\s*=|^\s*var\s+{re.escape(name)}\s*=)'
            elif element_type == 'class':
                pattern = rf'^\s*class\s+{re.escape(name)}\s*'
            else:
                return None, 0
        elif file_ext in ['.java', '.kt', '.cs']:
            if element_type in ['function', 'method']:
                pattern = rf'\s+{re.escape(name)}\s*\('
            elif element_type == 'class':
                pattern = rf'class\s+{re.escape(name)}\s*'
            else:
                return None, 0
        elif file_ext in ['.go']:
            # Go è¯­è¨€å‡½æ•°æ”¯æŒ
            if element_type in ['function', 'method']:
                pattern = rf'^\s*func\s+(?:\([^)]+\)\s*)?{re.escape(name)}\s*\('
            elif element_type == 'type':
                pattern = rf'^\s*type\s+{re.escape(name)}\s*'
            else:
                return None, 0
        elif file_ext in ['.rs']:
            # Rust è¯­è¨€å‡½æ•°æ”¯æŒ
            if element_type in ['function', 'method']:
                pattern = rf'^\s*(?:pub\s+)?fn\s+{re.escape(name)}\s*'
            elif element_type == 'struct':
                pattern = rf'^\s*(?:pub\s+)?struct\s+{re.escape(name)}\s*'
            elif element_type == 'enum':
                pattern = rf'^\s*(?:pub\s+)?enum\s+{re.escape(name)}\s*'
            else:
                return None, 0
        elif file_ext in ['.cpp', '.cc', '.cxx', '.hpp']:
            # C++ æ”¯æŒ
            if element_type in ['function', 'method']:
                pattern = rf'^\s*(?:template\s*<[^>]*>\s*)?(?:inline\s+)?(?:void|int|string|bool|auto|auto\s+|[\w:]+)\s+{re.escape(name)}\s*\('
            elif element_type == 'class':
                pattern = rf'^\s*class\s+{re.escape(name)}\s*(?::|{{)'
            else:
                return None, 0
        else:
            # é€šç”¨æ¨¡å¼
            pattern = rf'\b{re.escape(name)}\b'

        # æŸ¥æ‰¾å®šä¹‰è¡Œ
        start_line = None
        for i, line in enumerate(lines):
            if re.search(pattern, line):
                start_line = i
                break

        if start_line is None:
            return None, 0

        # æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©ä¸åŒçš„ç»“æŸè¡Œåˆ¤æ–­ç­–ç•¥
        brace_languages = ['.js', '.ts', '.jsx', '.tsx', '.html', '.htm', '.vue',
                          '.java', '.kt', '.cs', '.cpp', '.c', '.cc', '.cxx', '.h', '.hpp',
                          '.go', '.rs']

        if file_ext in brace_languages:
            # å¯¹äºå¤§æ‹¬å·è¯­è¨€ï¼Œä½¿ç”¨æ‹¬å·åŒ¹é…
            end_line = self._find_closing_brace(lines, start_line)
        else:
            # å¯¹äº Python ç­‰ç¼©è¿›è¯­è¨€ï¼Œä½¿ç”¨ç¼©è¿›åˆ¤æ–­
            end_line = self._find_end_by_indent(lines, start_line)

        snippet_lines = lines[start_line:end_line]
        snippet_content = "\n".join(snippet_lines)

        # éª¨æ¶æ¨¡å¼ï¼šå¯¹æå–åçš„ç‰‡æ®µæ‰§è¡Œéª¨æ¶æ¸…æ´—ï¼ˆè·¨è¯­è¨€ç»Ÿä¸€è¡Œä¸ºï¼‰
        if skeleton_mode and HAS_CODE_CLEANER:
            cleaned = extract_code_skeleton(snippet_content, file_ext)
            return cleaned, len(cleaned.splitlines())

        return snippet_content, len(snippet_lines)

    def _find_closing_brace(self, lines: List[str], start_line: int) -> int:
        """é€šè¿‡å¤§æ‹¬å·é…å¯¹æ‰¾åˆ°ä»£ç å—ç»“æŸä½ç½®ï¼ˆç”¨äº JavaScript/Java/C++ ç­‰ï¼‰"""
        brace_count = 0
        found_opening = False

        for i in range(start_line, len(lines)):
            line = lines[i]

            # è·³è¿‡å­—ç¬¦ä¸²å’Œæ³¨é‡Šä¸­çš„æ‹¬å·ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            # ç§»é™¤å•è¡Œæ³¨é‡Š
            if '//' in line:
                code_part = line[:line.index('//')]
            else:
                code_part = line

            # ç»Ÿè®¡æ‹¬å·
            for char in code_part:
                if char == '{':
                    brace_count += 1
                    found_opening = True
                elif char == '}':
                    brace_count -= 1

                # æ‰¾åˆ°åŒ¹é…çš„é—­æ‹¬å·
                if found_opening and brace_count == 0:
                    return i + 1

        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°åŒ¹é…çš„æ‹¬å·ï¼Œè¿”å›æ–‡ä»¶æœ«å°¾
        return len(lines)

    def _find_end_by_indent(self, lines: List[str], start_line: int) -> int:
        """é€šè¿‡ç¼©è¿›åˆ¤æ–­ä»£ç å—ç»“æŸä½ç½®ï¼ˆç”¨äº Python ç­‰ï¼‰"""
        base_indent = len(lines[start_line]) - len(lines[start_line].lstrip())
        end_line = start_line + 1

        for i in range(start_line + 1, len(lines)):
            line = lines[i]
            if line.strip() == "":
                continue

            current_indent = len(line) - len(line.lstrip())

            # å¦‚æœç¼©è¿›å›åˆ°åŒçº§æˆ–æ›´å°‘ï¼Œç»“æŸ
            if current_indent <= base_indent and line.strip():
                end_line = i
                break
        else:
            end_line = len(lines)

        return end_line

    def _is_core_file(self, file_path: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ ¸å¿ƒæ–‡ä»¶"""
        core_files = self.config.get('core_files', [])
        file_name = Path(file_path).name

        # æ£€æŸ¥å®Œæ•´è·¯å¾„æˆ–æ–‡ä»¶å
        return any(
            file_path == core or
            file_name == core or
            file_path.endswith(core)
            for core in core_files
        )

    def _format_file_section(self, file_info: Dict) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶æ®µè½"""
        md = f"### File: {file_info['path']}\n\n"
        md += f"```{file_info['language']}\n"
        md += file_info['content']
        md += "\n```\n\n---\n\n"
        return md

    def _format_snippet_section(self, snippet: Dict) -> str:
        """æ ¼å¼åŒ–ä»£ç ç‰‡æ®µæ®µè½"""
        if snippet['type'] == 'lines':
            md = f"### è¡Œ {snippet['range']}\n\n"
        else:
            md = f"### {snippet['type'].title()}: {snippet['name']}\n\n"

        md += "```\n"
        md += snippet['content']
        md += "\n```\n\n---\n\n"
        return md


def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(description='æ™ºèƒ½ä»£ç æ”¶é›†å·¥å…·')
    parser.add_argument('project_path', help='é¡¹ç›®è·¯å¾„')
    parser.add_argument('--mode', choices=['batch', 'snippets'], required=True,
                        help='è¿è¡Œæ¨¡å¼ï¼šbatchï¼ˆæ‰¹é‡å¯¼å…¥ï¼‰æˆ– snippetsï¼ˆç‰‡æ®µæå–ï¼‰')
    parser.add_argument('--files', nargs='+', help='æ–‡ä»¶åˆ—è¡¨ï¼ˆbatch æ¨¡å¼ï¼‰')
    parser.add_argument('--target', help='ç›®æ ‡æ–‡ä»¶ï¼ˆsnippets æ¨¡å¼ï¼‰')
    parser.add_argument('--ranges', help='æå–èŒƒå›´ JSONï¼ˆsnippets æ¨¡å¼ï¼‰')
    parser.add_argument('--intent', help='ç”¨æˆ·æ„å›¾æè¿°')
    parser.add_argument('--config', help='é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆJSONï¼‰')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶è·¯å¾„')
    parser.add_argument('--append', action='store_true',
                        help='è¿½åŠ æ¨¡å¼ï¼šå‘ç°æœ‰æ–‡ä»¶è¿½åŠ å†…å®¹ï¼Œè€Œä¸æ˜¯è¦†ç›–')
    parser.add_argument('--clean', choices=['none', 'comments', 'skeleton'], default='none',
                        help='ä»£ç æ¸…æ´—æ¨¡å¼ï¼šnone-ä¸å¤„ç†, comments-å»æ³¨é‡Š, skeleton-éª¨æ¶æ¨¡å¼')
    parser.add_argument('--no-junk-filter', action='store_true',
                        help='ç¦ç”¨åƒåœ¾æ–‡ä»¶è¿‡æ»¤ï¼ˆSTM32/Unityç­‰è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶ï¼‰')

    args = parser.parse_args()

    # åŠ è½½é…ç½®
    config = {}
    config_path = args.config
    
    if not config_path:
        # é»˜è®¤å°è¯•ä»çˆ¶ç›®å½•åŠ è½½ config.json
        default_config = Path(__file__).parent.parent / "config.json"
        if default_config.exists():
            config_path = str(default_config)

    if config_path and os.path.exists(config_path):
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)

    # åˆ›å»ºæ”¶é›†å™¨
    collector = CodeCollector(args.project_path, config)

    # åº”ç”¨å‘½ä»¤è¡Œæ¸…æ´—å‚æ•°
    if args.clean != 'none':
        collector.clean_mode = args.clean
    if args.no_junk_filter:
        collector.remove_junk = False

    # æ‰§è¡Œæ”¶é›†
    if args.mode == 'batch':
        if not args.files:
            print("é”™è¯¯ï¼šbatch æ¨¡å¼éœ€è¦æŒ‡å®š --files", file=sys.stderr)
            sys.exit(1)

        data = collector.batch_import(args.files)

    elif args.mode == 'snippets':
        if not args.target or not args.ranges:
            print("é”™è¯¯ï¼šsnippets æ¨¡å¼éœ€è¦æŒ‡å®š --target å’Œ --ranges", file=sys.stderr)
            sys.exit(1)

        ranges = json.loads(args.ranges)
        data = collector.extract_snippets(args.target, ranges)

    # å‘½ä»¤è¡Œå®æ—¶åé¦ˆï¼šæ£€æŸ¥è·³è¿‡çš„æ–‡ä»¶
    if 'skipped_files' in data and data['skipped_files']:
        print("\n" + "=" * 60)
        print("[è­¦å‘Š] æ£€æµ‹åˆ°ä»¥ä¸‹æ–‡ä»¶æœªèƒ½è‡ªåŠ¨æ”¶é›†ï¼š")
        print("=" * 60)
        for skipped in data['skipped_files']:
            print(f"\næ–‡ä»¶: {skipped['path']}")
            print(f"åŸå› : {skipped['reason']}")
            if 'size_kb' in skipped:
                print(f"æ–‡ä»¶å¤§å°: {skipped['size_kb']:.1f} KB")
            if 'lines' in skipped and skipped['lines']:
                print(f"é¢„ä¼°è¡Œæ•°: çº¦ {skipped['lines']} è¡Œ")
            print("å»ºè®®: ä½¿ç”¨ --mode snippets æŒ‡å®šå‡½æ•°/ç±»åæˆ–è¡Œå·èŒƒå›´æå–")
        print("=" * 60 + "\n")

    # ç”Ÿæˆ Markdownï¼ˆè¿½åŠ æ¨¡å¼æ—¶ä¼ å…¥å·²æœ‰æ–‡ä»¶è·¯å¾„ï¼‰
    markdown = collector.generate_markdown(
        data,
        args.intent or "",
        append_mode=args.append,
        existing_md_path=args.output if args.append else None
    )

    # è¾“å‡ºï¼ˆè¿½åŠ æ¨¡å¼ç»Ÿä¸€ä½¿ç”¨è¦†ç›–å†™å…¥ï¼Œå› ä¸ºå·²ç»åœ¨å†…å­˜ä¸­åˆå¹¶äº†ï¼‰
    if args.output:
        with open(args.output, 'w', encoding='utf-8') as f:
            f.write(markdown)

        action = "å·²åˆå¹¶æ›´æ–°åˆ°" if args.append else "å·²ä¿å­˜åˆ°"
        print(f"âœ… {action}: {args.output}")
    else:
        print(markdown)


if __name__ == "__main__":
    main()
